# æ—¥å¿—æ»šå­˜+æ—¥å¿—æ‰“å°æ§åˆ¶å°+é€‰ä¸¾+å¤šä»»åŠ¡ï¼ˆä¸€ä¸ªå¸¸é©»ä»»åŠ¡ã€ä¸€ä¸ªleaderæ‰§è¡Œä»»åŠ¡ï¼‰
```
package main

import (
	"context"
	"fmt"
	"io"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/labstack/gommon/log"
	"gopkg.in/natefinch/lumberjack.v2"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/tools/leaderelection"
	"k8s.io/client-go/tools/leaderelection/resourcelock"
)

// é€‰ä¸»æ ¸å¿ƒé…ç½®
const (
	leaderLockName      = "lyx-leader-lock"
	leaderLockNamespace = "monitoring"
	leaseDuration       = 15 * time.Second
	renewDeadline       = 10 * time.Second
	retryPeriod         = 2 * time.Second
	reElectDelay        = 1 * time.Second
	logFilePath         = "/file/go/src/test/log/leader-election.log"
)

var (
	k8sClient     kubernetes.Interface
	localIdentity string
)

func main() {
	// 1. åˆå§‹åŒ–æ—¥å¿—
	initLogger()
	log.Info("âœ… ç¨‹åºå¯åŠ¨ï¼Œåˆå§‹åŒ–LeaderElectionç»„ä»¶")

	// 2. åˆå§‹åŒ–K8så®¢æˆ·ç«¯
	var err error
	k8sClient, err = initK8sClient("/home/lyx/.kube/config")
	if err != nil {
		log.Fatal("âŒ K8så®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š", err.Error())
		os.Exit(1)
	}
	log.Info("âœ… K8så®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")

	// 3. æ ¹ä¸Šä¸‹æ–‡+é€€å‡ºä¿¡å·ç›‘å¬
	rootCtx, rootCancel := context.WithCancel(context.Background())
	defer rootCancel()
	registerSignalHandler(rootCancel)

	// 4. åˆå§‹åŒ–å®ä¾‹å”¯ä¸€æ ‡è¯†
	localIdentity = fmt.Sprintf("%s-%d", getHostName(), os.Getpid())
	log.Info("âœ… å®ä¾‹å”¯ä¸€æ ‡è¯†ï¼š", localIdentity)

	// 5. å¯åŠ¨å…¨å±€å¸¸é©»é€»è¾‘
	go defaultLogic()
	log.Info("âœ… å¸¸é©»é€»è¾‘defaultLogicå·²å¯åŠ¨")

	// 6. å¯åŠ¨é€‰ä¸»å¾ªç¯
	log.Info("âœ… å¯åŠ¨Leaderé€‰ä¸¾å¾ªç¯ï¼Œç­‰å¾…ç«é€‰...")
	lock := newLeaseLock()
	startLeaderElectionLoop(rootCtx, lock)
}

// ========== æ ¸å¿ƒæ¡†æ¶å‡½æ•° ==========
func initLogger() {
	logFile := &lumberjack.Logger{
		Filename:   logFilePath,
		MaxSize:    200,
		MaxBackups: 4,
		MaxAge:     7,
		Compress:   false,
		LocalTime:  true,
	}
	multiWriter := io.MultiWriter(os.Stdout, logFile)
	log.SetOutput(multiWriter)
	log.SetLevel(log.INFO)
	log.SetPrefix("[lyx-leader]")
}

func initK8sClient(kubeconfig string) (kubernetes.Interface, error) {
	var config *rest.Config
	var err error
	if kubeconfig == "" {
		kubeconfig = os.Getenv("KUBECONFIG")
	}

	if kubeconfig != "" {
		log.Infof("Loading kube client config from path %q", kubeconfig)
		config, err = clientcmd.BuildConfigFromFlags("", kubeconfig)
	} else {
		log.Infof("Using in-cluster kube client config")
		config, err = rest.InClusterConfig()
	}
	if err != nil {
		return nil, err
	}
	client, err := kubernetes.NewForConfig(rest.AddUserAgent(config, "lyx-leader"))
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºKubeClientå¤±è´¥: %w", err)
	}
	return client, nil
}

func registerSignalHandler(cancelFunc context.CancelFunc) {
	sigCh := make(chan os.Signal, 2)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT)
	go func() {
		<-sigCh
		log.Warn("âš ï¸ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼ˆCtrl+C/Killï¼‰ï¼Œå¼€å§‹ä¼˜é›…å…³åœ...")
		cancelFunc()

		time.Sleep(300 * time.Millisecond)
		log.Info("âœ… ç¨‹åºä¼˜é›…å…³åœå®Œæˆï¼Œé€€å‡ºè¿›ç¨‹")
		os.Exit(0)
	}()
}

func startLeaderElectionLoop(rootCtx context.Context, lock *resourcelock.LeaseLock) {
	for {
		// æ‰§è¡Œå•æ¬¡é€‰ä¸»
		leaderelection.RunOrDie(rootCtx, leaderelection.LeaderElectionConfig{
			Lock:            lock,
			ReleaseOnCancel: true,
			LeaseDuration:   leaseDuration,
			RenewDeadline:   renewDeadline,
			RetryPeriod:     retryPeriod,
			Callbacks: leaderelection.LeaderCallbacks{
				OnStartedLeading: onStartedLeading,
				OnStoppedLeading: onStoppedLeading,
			},
		})

		select {
		case <-rootCtx.Done():
			return
		case <-time.After(reElectDelay):
			log.Info(fmt.Sprintf("âš ï¸ ä¸¢å¤±Leaderèº«ä»½ï¼Œé‡æ–°ç«é€‰ï¼ˆé—´éš”%vsï¼‰", reElectDelay))
		}
	}
}

func newLeaseLock() *resourcelock.LeaseLock {
	return &resourcelock.LeaseLock{
		LeaseMeta: metav1.ObjectMeta{
			Name:      leaderLockName,
			Namespace: leaderLockNamespace,
		},
		Client: k8sClient.CoordinationV1(),
		LockConfig: resourcelock.ResourceLockConfig{
			Identity: localIdentity,
		},
	}
}

func defaultLogic() {
	log.Info("é€šç”¨å¸¸é©»é€»è¾‘defaultLogicå¯åŠ¨")
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			log.Info("defaultLogic - æ‰§è¡Œé€šç”¨ä»»åŠ¡")
		}
	}
}

func onStartedLeading(ctx context.Context) {
	log.Info("æˆåŠŸå½“é€‰Leaderï¼Œå¯åŠ¨ä¸“å±ä»»åŠ¡...")
	ticker := time.NewTicker(5 * time.Second)
	defer func() {
		ticker.Stop()
		log.Warn("âš ï¸ Leaderä¸“å±ä»»åŠ¡å·²åœæ­¢")
	}()
	for {
		select {
		case <-ctx.Done():
			log.Info("âœ… Leaderä»»åŠ¡ä¸Šä¸‹æ–‡å·²å–æ¶ˆï¼Œæ­£å¸¸é€€å‡º")
			return
		case <-ticker.C:
			Leaderlogic(ctx)
		}
	}
}
func Leaderlogic(ctx context.Context) {
	log.Info("ğŸ” Leaderä¸“å±é€»è¾‘ - æ‰§è¡Œæ£€æŸ¥ä»»åŠ¡")
}

// ä¸¢å¤±Leaderèº«ä»½åçš„æ¸…ç†é€»è¾‘
func onStoppedLeading() {
	log.Warn("âŒ å·²ä¸¢å¤±Leaderèº«ä»½ï¼Œé‡Šæ”¾Leaderèµ„æº")
}

// ========== å·¥å…·å‡½æ•° ==========
func getHostName() string {
	hostname, err := os.Hostname()
	if err != nil {
		log.Warn("è·å–ä¸»æœºåå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡è¯†", err)
		return "unknown-host"
	}
	return hostname
}
```